"""
SPSC Transcriber - Complete Legacy Architecture Port
Single Producer Single Consumer with ALL genius optimizations from legacy server
Implements smart batching, parallel processing, circuit breaker, and per-client aggregation
"""

import asyncio
import base64
import io
import json
import logging
import time
import wave
from collections import deque
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, NamedTuple, Any

logger = logging.getLogger(__name__)


class ChunkType(Enum):
    """Audio chunk priority types (exactly like legacy server)"""
    REALTIME = 1    # Highest priority - immediate processing
    BUFFERED = 2    # Normal priority - standard chunks
    BATCH = 3       # Lower priority - can wait for batching


@dataclass
class AudioChunk:
    """Audio chunk with metadata for SPSC queue (exactly like legacy)"""
    client_id: str
    audio_data: bytes
    chunk_id: str
    timestamp: float
    chunk_type: ChunkType = ChunkType.BUFFERED
    websocket: Optional[Any] = None
    session_data: Dict = field(default_factory=dict)

    def __lt__(self, other):
        """For priority queue - lower type value = higher priority"""
        return self.chunk_type.value < other.chunk_type.value


class CircuitBreaker:
    """Circuit breaker for resilient processing (exact legacy implementation)"""

    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half-open

    def is_open(self) -> bool:
        """Check if circuit breaker is open"""
        if self.state == "open":
            # Check if recovery timeout has passed
            if self.last_failure_time and \
               time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "half-open"
                logger.info("Circuit breaker entering half-open state")
                return False
            return True
        return False

    def record_success(self):
        """Record successful processing"""
        if self.state == "half-open":
            self.state = "closed"
            self.failure_count = 0
            logger.info("Circuit breaker closed - system recovered")

    def record_failure(self):
        """Record processing failure"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = "open"
            logger.warning(f"Circuit breaker opened after {self.failure_count} failures")


class SmartTranscriptionAggregator:
    """Intelligent transcription aggregator for natural text flow (legacy implementation)"""

    def __init__(self, silence_threshold_ms: int = 2000, sentence_breaks: bool = True):
        self.silence_threshold_ms = silence_threshold_ms
        self.sentence_breaks = sentence_breaks

        # Session text management (exactly like legacy)
        self.sentence_buffer = []
        self.current_paragraph = []
        self.all_paragraphs = []  # Store all paragraphs
        self.last_sent_index = 0  # Track what was already sent
        self.last_chunk_time = time.time()

        logger.info(f"SmartAggregator initialized: silence_threshold={silence_threshold_ms}ms, sentence_breaks={sentence_breaks}")

    def process_chunk(self, text: str, is_final: bool = False) -> Dict:
        """Process transcription chunk with intelligent aggregation"""
        current_time = time.time()
        time_since_last = (current_time - self.last_chunk_time) * 1000  # ms

        result = {
            'has_updates': False,
            'completed_paragraphs': [],
            'partial_sentence': '',
            'session_text': '',
            'paragraph_count': 0
        }

        if not text.strip() and not is_final:
            return result

        # Check for paragraph break due to silence
        if time_since_last > self.silence_threshold_ms and self.sentence_buffer:
            # Complete current paragraph
            completed_paragraph = ' '.join(self.current_paragraph + [self.sentence_buffer]).strip()
            if completed_paragraph:
                self.all_paragraphs.append(completed_paragraph)
                logger.info(f"Paragraph completed due to silence ({time_since_last:.0f}ms): {completed_paragraph[:50]}...")

            self.current_paragraph = []
            self.sentence_buffer = ""

        # Add new text
        if text.strip():
            if self.sentence_breaks:
                # Add to sentence buffer
                if self.sentence_buffer:
                    self.sentence_buffer += " " + text.strip()
                else:
                    self.sentence_buffer = text.strip()
            else:
                # Direct to current paragraph
                self.current_paragraph.append(text.strip())

        # Handle final processing
        if is_final:
            if self.sentence_buffer:
                self.current_paragraph.append(self.sentence_buffer)
                self.sentence_buffer = ""

            if self.current_paragraph:
                completed_paragraph = ' '.join(self.current_paragraph).strip()
                if completed_paragraph:
                    self.all_paragraphs.append(completed_paragraph)
                self.current_paragraph = []

        # Build result
        result['completed_paragraphs'] = self.all_paragraphs[self.last_sent_index:]
        result['partial_sentence'] = self.sentence_buffer
        result['session_text'] = '\n'.join(self.all_paragraphs)
        if result['partial_sentence']:
            result['session_text'] += '\n' + result['partial_sentence']
        result['paragraph_count'] = len(self.all_paragraphs)
        result['has_updates'] = bool(result['completed_paragraphs'] or result['partial_sentence'])

        # Update tracking
        self.last_sent_index = len(self.all_paragraphs)
        self.last_chunk_time = current_time

        return result

    def reset(self):
        """Reset aggregator for new session"""
        self.sentence_buffer = []
        self.current_paragraph = []
        self.all_paragraphs = []
        self.last_sent_index = 0
        self.last_chunk_time = time.time()


class SPSCAudioProcessor:
    """
    Complete Legacy SPSC Implementation - Single Producer Single Consumer
    ALL genius optimizations from legacy server included:
    - Zero-latency smart batching
    - Parallel sub-batch processing
    - Circuit breaker resilience
    - Per-client aggregators
    - Priority queue support
    - Backpressure control
    """

    def __init__(self, ai_factory, normalization_pipeline=None, data_registry=None):
        self.ai_factory = ai_factory
        self.normalization_pipeline = normalization_pipeline
        self.data_registry = data_registry

        # SPSC Configuration (exactly like legacy)
        self.queue_size = 50
        self.batch_size = 10  # Max chunks per batch
        self.batch_wait_ms = 50  # Max wait to fill batch (genius: short wait!)
        self.parallel_workers = 4  # Max concurrent tasks

        # Create SPSC queue
        self.audio_queue = asyncio.Queue(maxsize=self.queue_size)
        logger.info(f"SPSC: Using FIFO Queue (size={self.queue_size})")

        # Circuit breaker for resilience (exactly like legacy)
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60
        )

        # Smart aggregators per WebSocket connection (exactly like legacy)
        self.aggregators: Dict[str, SmartTranscriptionAggregator] = {}

        # Metrics (exactly like legacy)
        self.metrics = {
            'chunks_processed': 0,
            'chunks_dropped': 0,
            'queue_full_events': 0,
            'avg_queue_size': 0,
            'avg_processing_time': 0,
            'total_processing_time': 0,
            'batches_processed': 0,
            'parallel_tasks_executed': 0
        }

        # Consumer task management
        self.consumer_task = None
        self.shutdown_event = asyncio.Event()

        logger.info(f"SPSC: Batch processing enabled - size={self.batch_size}, wait={self.batch_wait_ms}ms, workers={self.parallel_workers}")

    async def start(self):
        """Start the SPSC consumer task"""
        if self.consumer_task is None:
            self.consumer_task = asyncio.create_task(self._consumer_loop())
            logger.info("SPSC: Consumer task started")

    async def stop(self):
        """Graceful shutdown (exactly like legacy)"""
        logger.info("SPSC: Shutting down...")
        self.shutdown_event.set()

        # Give consumer a chance to process remaining items (but don't wait forever)
        try:
            await asyncio.wait_for(self.audio_queue.join(), timeout=2.0)
        except asyncio.TimeoutError:
            logger.info(f"SPSC: Queue had {self.audio_queue.qsize()} items remaining")

        # Cancel consumer task
        if self.consumer_task:
            self.consumer_task.cancel()
            try:
                await self.consumer_task
            except asyncio.CancelledError:
                pass

        logger.info(f"SPSC: Shutdown complete. Processed {self.metrics['chunks_processed']} chunks")

    async def produce(self, audio_chunk: AudioChunk) -> bool:
        """
        Producer: Add audio chunk to queue (exactly like legacy)
        Returns True if successful, False if dropped
        """
        try:
            # Try to add with timeout to prevent blocking
            await asyncio.wait_for(
                self.audio_queue.put(audio_chunk),
                timeout=0.1  # 100ms timeout (like legacy)
            )

            # Update metrics (exactly like legacy)
            current_size = self.audio_queue.qsize()
            self.metrics['avg_queue_size'] = (
                self.metrics['avg_queue_size'] * 0.9 + current_size * 0.1
            )

            return True

        except asyncio.TimeoutError:
            # Queue full - implement backpressure (exactly like legacy)
            self.metrics['chunks_dropped'] += 1
            self.metrics['queue_full_events'] += 1
            logger.warning(f"SPSC: Dropped chunk for {audio_chunk.client_id} - queue full")
            return False

    async def _consumer_loop(self):
        """
        GENIUS LEGACY CONSUMER LOOP - Zero-latency smart batching
        Dramatically improves performance: 10 chunks processed in ~400ms instead of 4000ms
        KEY INSIGHT: Immediate processing when queue empty (no batching delay!)
        """
        logger.info("SPSC: Genius legacy consumer loop started")

        while not self.shutdown_event.is_set():
            try:
                # Collect a batch of chunks (GENIUS LEGACY LOGIC)
                batch = []
                batch_start_time = time.time()

                # Try to fill batch up to batch_size or until batch_wait_ms timeout
                while len(batch) < self.batch_size:
                    try:
                        # Calculate remaining time to wait
                        elapsed_ms = (time.time() - batch_start_time) * 1000
                        remaining_wait_ms = self.batch_wait_ms - elapsed_ms

                        if remaining_wait_ms <= 0:
                            break  # Timeout reached, process what we have

                        # Try to get a chunk with timeout
                        chunk = await asyncio.wait_for(
                            self.audio_queue.get(),
                            timeout=remaining_wait_ms / 1000.0
                        )

                        # Check for shutdown
                        if self.shutdown_event.is_set():
                            self.audio_queue.task_done()
                            break

                        batch.append(chunk)

                        # ðŸš€ GENIUS: If queue is empty, process what we have immediately!
                        # This is the KEY to zero-latency - no unnecessary waiting!
                        if self.audio_queue.empty():
                            break

                    except asyncio.TimeoutError:
                        break  # Timeout reached, process current batch

                # Process batch if we have chunks
                if batch:
                    await self._process_batch_parallel(batch)

                elif self.shutdown_event.is_set():
                    break  # Shutdown requested
                else:
                    # No chunks available, wait a bit
                    await asyncio.sleep(0.05)

            except asyncio.CancelledError:
                # Task cancelled, exit cleanly
                logger.info("SPSC: Consumer loop cancelled")
                break
            except Exception as e:
                logger.error(f"SPSC: Error in consumer loop: {e}")
                # Continue running even on errors (resilience)
                await asyncio.sleep(1)

    async def _process_batch_parallel(self, batch: List[AudioChunk]):
        """
        GENIUS LEGACY PARALLEL PROCESSING - Process batch with parallel sub-batches
        Splits batch into sub-batches and processes with multiple workers
        """
        batch_process_start = time.time()
        batch_size = len(batch)

        logger.info(f"SPSC: Processing batch of {batch_size} chunks with {self.parallel_workers} workers")

        # Process chunks in parallel with limited concurrency (exactly like legacy)
        # Split batch into sub-batches based on parallel_workers
        for i in range(0, batch_size, self.parallel_workers):
            sub_batch = batch[i:i + self.parallel_workers]

            # Create processing tasks for sub-batch
            tasks = [self._process_chunk_safe(chunk) for chunk in sub_batch]

            # Process sub-batch in parallel (GENIUS!)
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Handle any exceptions (resilience)
            for idx, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"SPSC: Chunk processing failed: {result}")
                    self.circuit_breaker.record_failure()
                else:
                    self.circuit_breaker.record_success()

        # Mark all chunks as done (exactly like legacy)
        for _ in batch:
            self.audio_queue.task_done()

        # Update metrics (exactly like legacy)
        batch_process_time = time.time() - batch_process_start
        self.metrics['chunks_processed'] += batch_size
        self.metrics['batches_processed'] += 1
        self.metrics['parallel_tasks_executed'] += len(batch)
        self.metrics['total_processing_time'] += batch_process_time
        self.metrics['avg_processing_time'] = (
            self.metrics['total_processing_time'] /
            self.metrics['chunks_processed']
        )

        # Log batch processing (exactly like legacy)
        avg_chunk_time = batch_process_time * 1000 / batch_size
        logger.info(
            f"SPSC: Batch processed {batch_size} chunks in {batch_process_time*1000:.1f}ms "
            f"({avg_chunk_time:.1f}ms per chunk), "
            f"Queue: {self.audio_queue.qsize()}/{self.queue_size}"
        )

    async def _process_chunk_safe(self, chunk: AudioChunk):
        """
        LEGACY GENIUS: Safe chunk processing with circuit breaker protection
        Includes smart aggregation and error resilience
        """
        try:
            # Check circuit breaker before processing
            if self.circuit_breaker.is_open():
                logger.warning(f"SPSC: Skipping {chunk.client_id} - circuit breaker open")
                return

            # Convert audio to WAV format (exactly like legacy)
            wav_data = self._convert_to_wav(chunk.audio_data)

            # Get dental prompts if available (like legacy)
            dental_prompt = await self._get_dental_prompts(chunk.client_id)

            # Transcribe audio using AI factory
            transcription_result = await self._transcribe_audio(wav_data, dental_prompt)

            # Get or create smart aggregator for this client (GENIUS PER-CLIENT!)
            aggregator = self._get_or_create_aggregator(chunk.client_id)

            # Process through aggregator for intelligent chunking (exactly like legacy)
            is_final = chunk.chunk_id.startswith('final_') or chunk.chunk_id.startswith('flush_')
            aggregation_result = aggregator.process_chunk(transcription_result.text, is_final)

            # Apply normalization (exactly like legacy)
            if self.normalization_pipeline and aggregation_result['has_updates']:
                normalized_text = self._normalize_text(aggregation_result['session_text'])
            else:
                normalized_text = aggregation_result['session_text']

            # Send results back to client (exactly like legacy)
            await self._send_aggregated_results(
                chunk,
                transcription_result,
                aggregation_result,
                normalized_text
            )

            # Record success for circuit breaker
            self.circuit_breaker.record_success()

        except Exception as e:
            logger.error(f"SPSC: Processing failed for {chunk.client_id}: {e}")
            self.circuit_breaker.record_failure()
            # Don't re-raise - continue processing other chunks (resilience)

    def _get_or_create_aggregator(self, client_id: str) -> SmartTranscriptionAggregator:
        """Get or create smart aggregator for client (exactly like legacy per-WebSocket aggregators)"""
        if client_id not in self.aggregators:
            # Create new aggregator with default settings
            self.aggregators[client_id] = SmartTranscriptionAggregator(
                silence_threshold_ms=2000,  # Default from legacy
                sentence_breaks=True
            )
            logger.info(f"SPSC: Created smart aggregator for client {client_id}")

        return self.aggregators[client_id]

    def _convert_to_wav(self, audio_data: bytes) -> bytes:
        """Convert PCM audio to WAV format (like legacy)"""
        # Create WAV file in memory
        wav_buffer = io.BytesIO()

        with wave.open(wav_buffer, 'wb') as wav_file:
            wav_file.setnchannels(1)  # Mono
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(16000)  # 16kHz
            wav_file.writeframes(audio_data)

        return wav_buffer.getvalue()

    async def _get_dental_prompts(self, client_id: str) -> str:
        """Get dental prompts from data registry (like legacy)"""
        if not self.data_registry:
            return "Dutch dental terminology"

        try:
            # Get admin user config for dental prompts
            admin_user_id = self.data_registry.loader.get_admin_id()
            config_data = await self.data_registry.get_config(admin_user_id)

            openai_prompt_config = config_data.get("openai_prompt", {})
            return openai_prompt_config.get("prompt", "Dutch dental terminology")
        except Exception as e:
            logger.warning(f"SPSC: Could not get dental prompts for {client_id}: {e}")
            return "Dutch dental terminology"

    async def _transcribe_audio(self, wav_data: bytes, prompt: str):
        """Transcribe audio using AI factory (exactly like legacy)"""
        # Create temporary file for transcription
        audio_file = io.BytesIO(wav_data)
        audio_file.name = "audio.wav"

        # Use AI factory for transcription
        asr_provider = await self.ai_factory.create_asr_provider()
        return await asr_provider.transcribe_audio(audio_file, language="nl", prompt=prompt)

    def _normalize_text(self, text: str) -> str:
        """Apply dental normalization (like legacy)"""
        if not self.normalization_pipeline or not text.strip():
            return text

        try:
            result = self.normalization_pipeline.normalize(text, language="nl")
            return result.normalized_text
        except Exception as e:
            logger.warning(f"SPSC: Normalization failed: {e}")
            return text

    async def _send_aggregated_results(self, chunk: AudioChunk, transcription_result, aggregation_result: Dict, normalized_text: str):
        """Send results back to client with smart aggregation (exactly like legacy)"""
        try:
            # Build response exactly like legacy format
            response = {
                "type": "transcription_result",
                "text": transcription_result.text,  # Current chunk
                "raw": transcription_result.text,  # Current chunk raw
                "normalized": normalized_text,     # Full session normalized
                "session_text": aggregation_result['session_text'],  # Full session with line breaks
                "language": transcription_result.language or "nl",
                "duration": getattr(transcription_result, 'duration', 0),
                "chunk_count": aggregation_result['paragraph_count'],
                "timestamp": time.time(),
                "chunk_id": chunk.chunk_id
            }

            # Send via WebSocket (like legacy)
            if chunk.websocket:
                await chunk.websocket.send_text(json.dumps(response))
                logger.debug(f"SPSC: Sent aggregated result to {chunk.client_id}")

        except Exception as e:
            logger.error(f"SPSC: Failed to send results to {chunk.client_id}: {e}")

    def cleanup_client(self, client_id: str):
        """Clean up client aggregator (exactly like legacy)"""
        if client_id in self.aggregators:
            del self.aggregators[client_id]
            logger.info(f"SPSC: Cleaned up aggregator for client {client_id}")

    def get_metrics(self) -> Dict:
        """Get SPSC processing metrics"""
        return {
            **self.metrics,
            "queue_size": self.audio_queue.qsize(),
            "max_queue_size": self.queue_size,
            "active_aggregators": len(self.aggregators),
            "circuit_breaker_state": self.circuit_breaker.state,
            "circuit_breaker_failures": self.circuit_breaker.failure_count
        }